{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten notebook jest oceniany półautomatycznie. Nie twórz ani nie usuwaj komórek - struktura notebooka musi zostać zachowana. Odpowiedź wypełnij tam gdzie jest na to wskazane miejsce - odpowiedzi w innych miejscach nie będą sprawdzane (nie są widoczne dla sprawdzającego w systemie).\n",
    "\n",
    "W szczególności zwróć uwagę, że usupełniłeś wszystkie miejsca `YOUR CODE HERE`, `WPISZ TWÓJ KOD TUTAJ`, \"YOUR ANSWER HERE\" lub \"WPISZ TWOJĄ ODPOWIEDŹ TUTAJ\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zaawansowane Przetwarzanie Języka Naturalnego\n",
    "# Laboratorium 2\n",
    "\n",
    "Pobierz zbiór danych Amazon \"Musical Instruments\" z [tej](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz) strony internetowej, a następnie wczytaj go poniższym kodem. Zwróć uwagę na wymaganą lokalizację pliku, tj. dwa katalogi wyżej - wynika to ze struktury plików w sprawdzarce, przepraszam za niedogodność.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    " \n",
    "x_text = []\n",
    "y = []\n",
    "with open('../../Musical_Instruments_5.json') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        x_text.append(data['reviewText'].lower().strip())\n",
    "        y.append(int(data['overall']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1 - przygotowanie danych\n",
    "W załadowanych listach `x_text` oraz `y` znajdują się odpowiednio teksty kolejnych opinii oraz oznaczenia klas. Klasą w tym wypadku jest liczba gwiazdek (ocena) produktu towarzysząca opinii. Zadanie klasyfikacji polega na przewidzeniu oceny na podstawie opinii pozostawionej w portalu.\n",
    "\n",
    "Aby zmniejszyć wymagania obliczeniowe do dalszych eksperymentów, ograniczymy zbiór danych jedynie do pierwszego tysiąca opinii.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text = x_text[:1000]\n",
    "y = y[:1000]\n",
    "train_end_idx=int(0.9 * len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedną z użytecznych operacji przygotowania tekstu do konstrukcji klasyfikatora jest zastąpienie poszczególnych tokenów ich indeksami. Chociaż w praktyce ten proces często następuje dopiero po szeregu etapów przetwarzania tekstu takich jak tokenizacja, lematyzacja czy stemming - w tym ćwiczeniu wyodrębnimy tokeny rozdzielając tekst znakiem spacji.\n",
    "\n",
    "Klasyfikator powinien obsługiwać także słowa, które nie występowały w zbiorze uczącym. Podstawową techniką obsługi takich słów jest wprowadzenie specjalnego tokenu UNK, obsługującego nieznane słowa. W tym celu usuwa się ze zbioru danych pewną liczbę najrzadszych słów i zastępuje się je tokenami UNK.\n",
    "\n",
    "Zbuduj słownik `w2i` mapujący tokeny na kolejne indeksy tj. liczby naturalne. Pomiń tokeny występujące w zbiorze uczącym 5 lub mniej razy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f866dd9918a27f9ba2f31ef62a061fa1",
     "grade": false,
     "grade_id": "cell-87edb8b6e5279a0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))\n",
    "UNK = w2i[\"<unk>\"] #Przypisz indeks tokenowi UNK\n",
    "\n",
    "tokens = []\n",
    "for x in x_text:\n",
    "    tokens += x.split(' ')\n",
    "tokens_counter = Counter(tokens)\n",
    "for k, v in tokens_counter.items():\n",
    "    if v > 5:\n",
    "        w2i[k]\n",
    "\n",
    "n_words = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b1000affcd1be506cad1e3a22a63cf1",
     "grade": true,
     "grade_id": "cell-59701c5db1041735",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po zbudowaniu słownika `w2i`, przekonwertujmy nasz zbiór danych z listy słów na listę indeksów słów. Od razu podzielimy zbiór na część uczącą i część testową, a także przekonwertujemy klasy na indeksy klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: UNK, w2i) # Domyślną wartością słownika jest UNK, \n",
    "         #chociaż w2i będzie zawierał wpisy do wszystkich słów to nowym tokenom będzie przypisywał indeks UNK\n",
    "class2i = defaultdict(lambda: len(class2i))\n",
    "        # mapuj klasy na indeksy klas\n",
    "    \n",
    "def read_dataset(start_idx,end_idx):\n",
    "    for i, text in enumerate(x_text[start_idx:end_idx]):\n",
    "        yield ([w2i[x] for x in text.split(\" \")], class2i[y[i]])\n",
    "        \n",
    "train = list(read_dataset(0, train_end_idx))\n",
    "dev = list(read_dataset(train_end_idx, len(y)))\n",
    "n_class = len(class2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389 5\n"
     ]
    }
   ],
   "source": [
    "print(n_words, n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2 - pierwszy model klasyfikacji tekstu w PyTorch\n",
    "Podstawową strukturą danych w PyTorch jest tensor, na którym możesz wykonywać analogiczne operacje jak na macierzach `numpy`. Podstawową metodą stworzenia tensora jest wywołanie konstruktora `torch.tensor` na liście liczb. Istnieją także inne konstruktory tensorów, analogiczne do `numpy`. Można też na nich operować za pomocą standardowych operatorów, indeksowania, i odpowiedników innych funkcji znanych z `numpy`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[0.3391, 0.4821, 0.2033],\n",
      "        [0.8161, 0.6028, 0.2198],\n",
      "        [0.9412, 0.0554, 0.4509]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1,2,3]))\n",
    "print(torch.rand( (3,3) ))\n",
    "print(torch.ones( (3,3) ))\n",
    "print(2 * torch.ones( (3,3) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dlaczego więc korzystamy z PyTorch, a nie z biblioteki `numpy` skoro tensory wydają się mieć analogiczną funkcjonalność do poznanych uprzednio macierzy? Powodów jest oczywiście wiele, m.in. możliwość przeniesienia obliczeń na kartę graficzną (technologia CUDA), ale z punktu widzenia naszego ćwiczenia kluczowa jest funkcjonalność automatycznego liczenia gradientów. W przypadku konstrukcji sieci neuronowej czy modelu liniowego, w PyTorch nie jest konieczne samodzielne wyprowadzanie i implementowanie gradientu, gdyż biblioteka zrobi to za nas automatycznie.\n",
    "\n",
    "Wyznaczanie gradientów odbywa się za pomocą algorytmu wstecznej propagacji, który ma dwie fazy: *forward* i *backward*. Faza *forward* polega na policzeniu wyniku funkcji, a faza *backward* wyznacza gradienty wszystkich jej parametrów.\n",
    "\n",
    "W celu poznania tej funkcjonalności policzymy pochodne cząstkowe prostej funkcji kwadratowej:\n",
    "$$result = x_1^2 + x_2^2+ x_3^2$$\n",
    "której pochodne cząstkowe mają postać $2x_i$.\n",
    "\n",
    "Rozpocznijmy implementacje tej funkcji od stworzenia 3-elementowego wektora zmiennych `x`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.,2.,3.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak pewnie zauważyłeś, w konstruktorze użyliśmy dodatkowego parametru `requires_grad`. Domyślnie wykonanie operacji na dowolnym tensorze nie traktuje się jako części fazy `forward`, gdyż nie do wszystkich tensorów użytych w kodzie będziemy potrzebować wartości pochodnych. Aby zasygnalizować, że dla danej zmiennej konieczne jest zapisywanie informacji o wykonywanych na niej operacjach, należy ustawić wartość jej parametru `requires_grad` na `True`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przejdźmy do policzenia wartości wyżej zdefiniowanej funkcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (x**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensory posiadają parametr `.grad`, który przechowuje informacje o wyznaczonym gradiencie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tej chwili, pomimo obliczenia wartości zmiennej `result`, wartość gradientu nie jest policzona, gdyż nie poinformowaliśmy biblioteki o zakończeniu fazy `forward` i konieczności wykonania fazy `backward`. Możemy to zrobić poprzez wykonanie funkcji `backward()` na obliczonej wartości funkcji (funkcję tę można wywołać tylko na skalarnym wyniku!).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę, że wektor `x.grad`, zgodnie z naszymi oczekiwaniami, zawiera wartości pochodnej cząstkowej tj. `2x`. Spróbujmy jeszcze raz, licząc pochodną po logarytmie z `result`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = torch.log(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\micha\\ai-university-labs\\Advanced Natural Language Processing\\lab2\\lab2.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/micha/ai-university-labs/Advanced%20Natural%20Language%20Processing/lab2/lab2.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result2\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[1;32mc:\\Users\\micha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\micha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "result2.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niestety operacja się nie powiodła. Przed wykonaniem kolejnej fazy *backward* należy - upraszczając - wykonać fazę *forward*. Nasze poprzednie operacje konstruowały fazę *forward* od parametrów `x` aż do zmiennej z wynikiem, jednakże przy wykonaniu fazy *backward* została zwolniona pamięć przechowująca informacje o kolejno wykonywanych operacjach na tych zmiennych (graf obliczeń). Kolejna operacja została wykonana bezpośrednio na tensorze `result`, konstruując fazę *forward* od `result` do `result2`, jednak zabrakło grafu obliczeń od parametrów `x`.\n",
    "\n",
    "Uruchomienie poniższego kodu, z operacjami rozpoczynającymi się od `x`, zakończy się obliczeniem gradientu z sukcesem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1429, 4.2857, 6.4286])\n"
     ]
    }
   ],
   "source": [
    "result = (x**2).sum()\n",
    "result2 = torch.log(result)\n",
    "result2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy poprawność uzyskanego wyniku. Zmienna $result= 1^2+2^2+3^2=14$, a pochodna z logarytmu naturalnego to $\\frac{1}{x}$. W związku z tym:\n",
    "$$\\frac{\\partial }{\\partial x_1} \\log result = \\frac{1}{result} \\cdot \\frac{\\partial }{\\partial x_1} result = \\frac{1}{result} 2x_1 $$\n",
    "Przy naszych wartościach $x$ równa się to $\\frac{1}{14}\\cdot 2 = 0,1428$. Łatwo zauważyć, że wynik znajdujący się w tensorze `x.grad` jest błędny, a konkretnie za duży o 2 jednostki.\n",
    "\n",
    "Stało się tak dlatego, że gradient z kolejnych faz `backward` jest akumulowany w parametrze `.grad` (poprzednia wartość policzonej pochodnej cząstkowej wynosiła właśnie 2). Takie zachowanie biblioteki może być bardzo użyteczne w sytuacji gdy chcemy w zmiennej zagregować gradienty funkcji celu liczonych na kolejno przetwarzanych instancjach lub przy treningu modelu z wieloma funkcjami celu; tutaj jednak doprowadziło to do błędnego wyniku. Z tego powodu bardzo ważne jest pamiętanie o wyzerowaniu wartości gradientów przed przystąpieniem do kolejnych obliczeń.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1429, 0.2857, 0.4286])\n"
     ]
    }
   ],
   "source": [
    "result = (x**2).sum()\n",
    "result2 = torch.log(result)\n",
    "result2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę na konwencję biblioteki PyTorch - jeśli nazwa funkcji zakończona jest podkreślnikiem to taka operacja jest wykonywana `in-place`. (np. `x.add(5)` - `x` nadal ma stałą wartość, `x.add_(5)` wartość `x` zwiększono o 5).\n",
    "\n",
    "Zaimplementujmy podstawowy algorytm uczący w PyTorch. Będzie to prosta sieć neuronowa składająca się z:\n",
    "- macierzy zanurzeń $C$, przetwarzającej indeksy słów na odpowiednie reprezentacje wektorowe, \n",
    "- operacji uśredniania tych zanurzeń do jednego zanurzenia (average pooling over time) \n",
    "- oraz jednej warstwy liniowej (softmax) zwracającej wynik.\n",
    "\n",
    "Algorytmem uczącym będzie SGD optymalizujące entropię krzyżową, czyli dla kolejnych instancji uczących będziemy wykonywać:\n",
    "$$parametry = parametry - \\eta \\nabla f\\_celu$$\n",
    "Implementacja ta będzie wyjątkowo prosta, gdyż gradient funkcji celu ($\\nabla f\\_celu$) zostanie obliczony automatycznie przez PyTorch. Ponadto entropia krzyżowa jest już zaimplementowana w PyTorch `F.cross_entropy(logits, target)`. Zwróć uwagę, że argumentem tej funkcji są wartości logitów (nie trzeba implementować funkcji softmax przetwarzającej wartości logitów na prawdopodobieństwa).\n",
    "\n",
    "**UWAGA** W implementacji nie należy używać gotowych implementacji SGD czy warstw sieci neuronowych w PyTorch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszym krokiem w implementacji będzie zaimplementowanie samego modelu. Należy zainicjalizować macierz $C$ przechowującą w wierszach zanurzenia dla kolejnych słów (liczba słów to `n_words`, wymiarowość zanurzenia określ na 20) oraz macierz $W$ przechowującą parametry warstwy liniowej, zwracającej wartości logitów dla każdej z klas (liczba klas to `n_class`). Macierz $W$ w dodatkowej kolumnie powinna też przechowywać wartości wyrazów wolnych (bias). Wartości zainicjalizuj losowo `torch.rand`. Pamiętaj, że dla tych macierzy będziesz potrzebował wyznaczyć potem wartości gradientów.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "134cf7cd8660133b178caff1ca2fcdfc",
     "grade": false,
     "grade_id": "cell-9ba79ce47d0c6a5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "EMBEDDING_SIZE = 20\n",
    "C = torch.rand(size=(n_words, EMBEDDING_SIZE), requires_grad=True)\n",
    "W = torch.rand(size=(n_class, EMBEDDING_SIZE+1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "084514b8fbf6ffa4f784b5b398bd3624",
     "grade": true,
     "grade_id": "cell-b3436825d33d5941",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj funkcję `simple_model`, której argumentem będzie instancja testowa (jest to więc lista indeksów słów występujących w tekście), a na której wyjściu będzie wektor `n_class`-elementowy zawierający obliczone wartości logitów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "280f8962c6b7f20c718a51b6ecc294a4",
     "grade": false,
     "grade_id": "cell-e2668a9abacbc78a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def simple_model(x):\n",
    "    doc_embedding = C[x].mean(0)\n",
    "    doc_with_bias = torch.cat([torch.ones(1),doc_embedding]) # Skonkatenowanie 1 z uzyskaną reprezentacją (bias)\n",
    "    return W @ doc_with_bias # Obliczenie wartości logitów (tj. warstwa liniowa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj algorytm SGD w poniższej pętli. Pętla ta iteruje po zbiorze uczącym oraz dla każdej instancji oblicza wartość funkcji celu. Twoje zadania:\n",
    "- Policz gradienty (faza *backward*)\n",
    "- Zaimplementuj aktualizacje $W$ i $C$ wg. wzoru na SGD. Operacje modyfikujące $W$ i $C$ musisz wykonać w środku klauzuli `with torch.no_grad():`, aby nie śledzić z tych operacji gradientów.\n",
    "- Pamiętaj o wyczyszczeniu gradientów (zarówno w $W$ jak i $C$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80acc452845b941b086f5aa582f26d87",
     "grade": false,
     "grade_id": "cell-aeedd0f5183bd781",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=0.9474\n",
      "iter 1: avg. train loss=0.9179\n",
      "iter 2: avg. train loss=0.8982\n",
      "iter 3: avg. train loss=0.8494\n",
      "iter 4: avg. train loss=0.8227\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        pred = simple_model(words)\n",
    "        loss = F.cross_entropy(pred.reshape(1,-1), torch.tensor(tag).reshape(1))\n",
    "        # Loss zawiera wartość funkcji celu dla przykładu, wykonaj backpropagation\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            train_loss += loss\n",
    "            W -= eta * W.grad\n",
    "            C -= eta * C.grad\n",
    "            W.grad.zero_()\n",
    "            C.grad.zero_()\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Dlaczego w bibliotekach do głębokiego uczenia maszynowego, takich jak PyTorch, implementuje się funkcje entropii krzyżowej tak, aby przyjmowała na wejście wartości logitów zamiast prawdopodobieństw z softmax?\n",
    "- Na wykładzie pokazywaliśmy warstwę zanurzeń jako warstwę mnożącą macierz $C$ przez wektor \"1 z n\", można ją jednak także zaimplementować jako operację odczytu odpowiedniego wiersza z macierzy. Jakie są wady i zalety obu tych sposobów implementacji?\n",
    "\n",
    "Odpowiedź na pierwszą kropkę umieść poniżej.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5158398dc380a8a01088a617375d8f4c",
     "grade": true,
     "grade_id": "cell-c685493684bda3e3",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "* Ze względu na numeryczną stabilność - obliczenia na logitach mogą być bardziej stabilne niż bezpośrednio na prawdopodobieństwach po softmaxie. Umożliwiają zastosowanie tzw. \"log-sum-exp trick\", który umożliwia normalizację obliczanych wartości gradientu w celu uniknięcia wykonywania operacji matematycznych na zbyt dużych lub zbyt małych liczbach ze względu na charakter funkcji wykładniczej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3 - wykorzystanie nn.Module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch jako biblioteka do głębokiego uczenia maszynowego oferuje nam kilka udogodnień w implementowaniu modeli uczących się, aby jeszcze bardziej uprościć ich implementację. Większość z tych udogodnień związanych z jest z reprezentowaniem modeli uczących się jako obiektów dziedziczących po `torch.nn.Module`. W obiekcie takim powinniśmy zaimplementować co najmniej konstruktor, inicjalizujący parametry modelu ($W$ i $C$), oraz funkcję `forward` obliczającą wynik modelu (we wcześniejszym zadaniu nazywaliśmy ją `simple_model`). Ponadto moduł `torch.nn` oferuje gotowe implementacje zarówno warstwy liniowej jak i warstwy zanurzeń.\n",
    "\n",
    "Przeanalizuj poniższą implementację modelu z poprzedniego zadania.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, n_words, emb_size, n_class):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(n_words, emb_size)\n",
    "        self.linear = torch.nn.Linear(in_features=emb_size, out_features=n_class, bias=True)\n",
    "        torch.nn.init.uniform_(self.embedding.weight, -0.25, 0.25)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words)                 \n",
    "        h = emb.mean(dim=0)                         \n",
    "        h = torch.reshape(h, (1,-1))\n",
    "        out = self.linear(h)              \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oprócz tego, że uzyskaliśmy elegancki obiekt reprezentujący nasz model, nie wydaje się by powyższa implementacja była krótsza czy prostsza od tej, którą uzyskaliśmy w poprzednim zadaniu bez dobrodziejstw `nn.Module`. Co zatem zyskaliśmy?\n",
    "\n",
    "Przy implementacji modeli z dużą liczbą warstw, szczególnie uciążliwe byłoby implementowanie kolejnych linijek kodu zerujących gradienty wszystkich macierzy wag, oraz wykonywanie na nich kroków algorytmu SGD. W naszej implementacji każda macierz parametrów to dwie linijki kodu! Jednak modele dziedziczące po `torch.nn.Module` i stworzone poprzez dedykowane warstwy neuronowe posiadają gotową funkcję `parameters()` zwracającą kolejne macierze parametrów modelu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0640,  0.0570, -0.0421,  ..., -0.1135, -0.0396,  0.0020],\n",
      "        [-0.1931,  0.2119, -0.0051,  ..., -0.1836,  0.1683,  0.1421],\n",
      "        [-0.1387,  0.0166,  0.0785,  ...,  0.0705, -0.0639,  0.1139],\n",
      "        ...,\n",
      "        [ 0.0484,  0.1090,  0.1421,  ..., -0.0402,  0.1829, -0.1152],\n",
      "        [-0.0120,  0.0879, -0.2020,  ..., -0.1967,  0.2361, -0.0584],\n",
      "        [ 0.1255,  0.0492,  0.2449,  ..., -0.2322, -0.1978,  0.1514]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-3.7073e-02,  3.3903e-01, -3.0538e-01,  2.0920e-01,  2.7132e-01,\n",
      "         -4.5277e-01,  4.5895e-01, -7.1795e-03,  1.3854e-01, -4.6293e-01,\n",
      "         -2.7466e-01,  2.1918e-01,  4.2122e-01, -3.0841e-01,  2.6265e-02,\n",
      "         -3.9379e-01, -4.5149e-01,  1.2289e-01,  1.2571e-01,  4.0232e-01],\n",
      "        [ 5.9378e-02,  3.4102e-01,  1.5140e-01, -6.7237e-02, -1.0454e-01,\n",
      "          8.8353e-02,  7.8004e-02, -4.3586e-01,  2.2841e-01, -3.0769e-01,\n",
      "          3.7713e-01, -6.5301e-02, -2.1204e-01, -2.8390e-01, -3.5849e-01,\n",
      "         -2.1746e-02,  4.6781e-01,  3.2465e-01,  4.5824e-01,  1.9807e-01],\n",
      "        [ 1.3369e-04, -3.5282e-01, -2.1178e-02,  1.8067e-01, -1.9841e-01,\n",
      "         -4.7105e-01,  2.6900e-01, -3.5214e-02,  4.4408e-01, -2.9477e-01,\n",
      "         -9.1807e-02,  3.7109e-01,  1.6984e-01,  4.1651e-01, -1.3848e-01,\n",
      "         -1.4561e-01,  1.0993e-02,  4.2738e-01, -2.1193e-01, -2.8798e-02],\n",
      "        [ 1.6853e-01, -4.2832e-01, -2.4356e-01, -9.3731e-03, -4.5536e-03,\n",
      "          9.0504e-02, -3.1265e-01, -2.3308e-01, -6.8793e-02, -1.1077e-01,\n",
      "         -2.7944e-01,  4.4275e-01, -1.4446e-01,  3.7992e-01, -4.3643e-01,\n",
      "         -1.6754e-01,  1.9376e-01,  4.0618e-01, -2.4117e-01, -1.1333e-02],\n",
      "        [-3.1191e-01,  4.0318e-01, -4.5989e-01, -4.1893e-01,  1.6053e-01,\n",
      "          1.9122e-01, -4.7072e-01, -2.2237e-01, -4.3575e-01, -3.7500e-01,\n",
      "         -3.6213e-01, -7.8667e-02,  8.1425e-02, -4.0449e-01, -2.0662e-01,\n",
      "          3.8951e-02,  3.2775e-01,  1.6693e-02, -1.8595e-01,  2.0248e-01]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0470,  0.1354, -0.1346, -0.0223,  0.0708], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model = SimpleModel(n_words, EMBEDDING_SIZE, n_class)\n",
    "print([i for i in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jest to niezwykle wygodne, bo implementacja algorytmu SGD może przeiterować po tej liście parametrów i dla każdej z nich wykonać aktualizację ich wartości. Fakt, że taka lista jest tworzona automatycznie pozbawia nas ryzyka, że zwyczajnie o którejś macierzy parametrów czy wektorze wyrazów wolnych najzwyczajniej zapomnimy. \n",
    "\n",
    "Podobnie można zaimplementować pętlę zerującą gradienty wszystkich parametrów. Modele oferują nawet gotową taką funkcję `model.zero_grad()`, która iteruje po parametrach zerując ich gradienty. \n",
    "\n",
    "Zmodyfikuj implementację SGD z poprzedniego zadania, tak aby wykorzystywała `zero_grad()` i `parameters()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5f2f3a6cb7d7b2d2e34e63a2b54004b",
     "grade": false,
     "grade_id": "cell-87d2dfc0c801725a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=0.9839\n",
      "iter 1: avg. train loss=0.9420\n",
      "iter 2: avg. train loss=0.9172\n",
      "iter 3: avg. train loss=0.8961\n",
      "iter 4: avg. train loss=0.8623\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "          pred = model(torch.tensor(words))\n",
    "          loss = F.cross_entropy(pred.reshape(1,-1), torch.tensor(tag).reshape(1))\n",
    "          loss.backward()\n",
    "          with torch.no_grad():\n",
    "                train_loss += loss\n",
    "                for p in model.parameters():\n",
    "                    p -= eta * p.grad\n",
    "                model.zero_grad()                    \n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo moduł `torch.nn` oferuje także od razu zaimplementowane optymalizatory, w tym SGD. W konstruktorze optymalizatora należy podać listę optymalizowanych przez niego parametrów, a następnie wywołać na nim procedurę `step()` wykonującą krok algorytmu optymalizacyjnego tj. aktualizację wartości zmiennych przy użyciu gradientu. W tej sytuacji nie musisz się martwić o umieszczanie kodu zmieniającego parametry w `with torch.no_grad()` - optymalizator sam to zrobi! Optymalizator również oferuje funkcję `zero_grad()`, zerującą gradienty zmiennych wskazanych do optymalizacji.\n",
    "\n",
    "Zmodyfikuj kod z poprzedniego zadania, tak aby wykorzystywał optymalizator SGD zaimplementowany w `torch.optim`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92b92a0626a6c533b3b61aa2e6c18ddd",
     "grade": false,
     "grade_id": "cell-5588a40beb11d256",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=0.8251\n",
      "iter 1: avg. train loss=0.8039\n",
      "iter 2: avg. train loss=0.7876\n",
      "iter 3: avg. train loss=0.7537\n",
      "iter 4: avg. train loss=0.7120\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "epochs = 5\n",
    "eta = 0.5  # prędkość uczenia\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=eta)\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        pred = model(torch.tensor(words))\n",
    "        loss = F.cross_entropy(pred.reshape(1,-1), torch.tensor(tag).reshape(1))\n",
    "        loss.backward()\n",
    "        train_loss += loss\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Przeanalizuj dokładnie powyższy kod i przechodząc linia po linii, wyjaśnij co one robią z punktu widzenia treningu modelu.\n",
    "- Zastanów się jak wyglądałaby Twoja własna implementacja klasy `optim.SGD`.\n",
    "- Prześledź jeszcze raz implementację modelu neuronowego - pewnie w niedługim czasie będziesz implementował znacznie bardziej skomplikowane modele, tym bardziej warto je dobrze prześledzić!\n",
    "- Czym różni się zaimplementowana architektura od głębokiej sieci uśredniającej?\n",
    "- Na wykładzie korzystaliśmy z macierzy zanurzeń w modelach języka. Tutaj warstwa zanurzeń pojawiła się bezpośrednio w modelu klasyfikacji. Czy w uzyskanych w ten sposób zanurzeniach (zakładając dobry dobór hiperparamerów, dodanie regularyzacji itd.) zaobserwowalibyśmy podobne zależności jak te uzyskane za pomocą modelu języka? Jeśli nie, obserwacji jakich zależności między słowami spodziewałbyś się w tej reprezentacji? Skąd biorą się różnice?\n",
    "\n",
    "Odpowiedź na ostatnią kropkę umieść poniżej.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "731e0439a761d52c3124060a972eaf31",
     "grade": true,
     "grade_id": "cell-50c2ecade40bcd7f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "* Nie zaobserwowalibyśmy podobnych zależności, gdyż mamy w tym przypadku do czynienia z innymi problemami. Uzyskane zależności w macierzy zanurzeń pochodzącej z modelu języka ukazywałyby zależności między słowami/grupami słów o podobnym znaczeniu, występujące blisko siebie w tekście. W przypadku modelu klasyfikacji można będzie zauważyć zależności charakterystyczne dla danego problemu klasyfikacyjnego np. pomiędzy pozytywnymi przykładami w przypadku problemu analizy wydźwięku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73080ad0900acee85ddd2dd6ff52a07d",
     "grade": false,
     "grade_id": "cell-a81ef968001b7310",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Zadanie 4\n",
    "Wykorzystując wiedzę z poprzedniego zadania zaimplementuj prostą architekturę splotową do klasyfikacji tekstu i wytrenuj ją. Do jej wykonania może być przydatna klasa `torch.nn.Conv1d` i funkcja `torch.nn.ReLU` (zapoznaj się z ich dokumentacją w Internecie). Jako funkcji redukcji użyj funkcji maksimum (over time).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8e23fa15d0164d9a5260ab5ea518263",
     "grade": false,
     "grade_id": "cell-dbb714252ae2ebc7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: avg. train loss=0.9689\n",
      "iter 1: avg. train loss=0.9136\n",
      "iter 2: avg. train loss=0.8740\n",
      "iter 3: avg. train loss=0.8417\n",
      "iter 4: avg. train loss=0.7632\n",
      "iter 5: avg. train loss=0.6689\n",
      "iter 6: avg. train loss=0.5217\n",
      "iter 7: avg. train loss=0.3877\n",
      "iter 8: avg. train loss=0.3326\n",
      "iter 9: avg. train loss=0.2303\n"
     ]
    }
   ],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, n_words, emb_size, num_filters, window_size, ntags):\n",
    "        super(CNN, self).__init__()\n",
    "        self.emb_size, self.num_filters = emb_size, num_filters\n",
    "        self.embedding = torch.nn.Embedding(n_words, emb_size)\n",
    "        self.conv = torch.nn.Conv1d(in_channels=emb_size, out_channels=num_filters, kernel_size=window_size)\n",
    "        self.linear = torch.nn.Linear(in_features=num_filters, out_features=ntags, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.conv.weight)\n",
    "        torch.nn.init.uniform_(self.embedding.weight, -0.25, 0.25)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, words):\n",
    "        x = self.embedding(words).reshape(1, self.emb_size, -1)\n",
    "        x = self.conv(x).reshape(self.num_filters, -1)\n",
    "        x = F.relu(x).max(1).values\n",
    "        return self.linear(x)\n",
    "\n",
    "epochs = 10\n",
    "eta = 0.05      \n",
    "\n",
    "model = CNN(n_words=n_words, emb_size=EMBEDDING_SIZE, num_filters=128, window_size=2, ntags=n_class)\n",
    "optimizer = optim.SGD(model.parameters(), lr=eta)\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    for words, tag in train:\n",
    "        pred = model(torch.tensor(words))\n",
    "        loss = F.cross_entropy(pred.reshape(1,-1), torch.tensor(tag).reshape(1))\n",
    "        loss.backward()\n",
    "        train_loss += loss\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(\"iter %r: avg. train loss=%.4f\" % (i, train_loss / len(train)))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
